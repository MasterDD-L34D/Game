{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96942bf2",
   "metadata": {},
   "source": [
    "# Loadout recommender training pipeline\n",
    "\n",
    "Questo notebook costruisce la pipeline di training per il modello di raccomandazione dei loadout, includendo fasi di feature engineering, validazione e salvataggio del modello addestrato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e46a463",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Importiamo le librerie necessarie e definiamo i percorsi degli artefatti. Il notebook Ã¨ stato pensato per funzionare sia con dati reali (se disponibili) sia con un dataset sintetico di fallback per poter validare l'intera pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfacf709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from joblib import dump\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = Path(\"../../data/derived/loadout_sessions.parquet\")\n",
    "MODEL_OUTPUT = Path(\"../models/loadout_recommender.joblib\")\n",
    "MODEL_OUTPUT.parent.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ede97f",
   "metadata": {},
   "source": [
    "## 2. Data ingestion\n",
    "Carichiamo il dataset da `data/derived/loadout_sessions.parquet`. Nel caso il file non sia presente generiamo un dataset sintetico basato sulle distribuzioni osservate nelle telemetrie interne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_synthetic_dataset(rows: int = 5000) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "    maps = [\"Crimson Dunes\", \"Azure Ruins\", \"Nebula Outpost\", \"Fungal Labyrinth\"]\n",
    "    weapon_types = [\"burst_rifle\", \"plasma_bow\", \"scatter_shot\", \"ion_blade\"]\n",
    "    companion = [\"medic_drone\", \"shield_mender\", \"scout_beetle\", \"none\"]\n",
    "    df = pd.DataFrame({\n",
    "        \"player_id\": rng.integers(100000, 999999, size=rows),\n",
    "        \"skill_rating\": rng.normal(1800, 220, size=rows).clip(900, 2800),\n",
    "        \"map\": rng.choice(maps, size=rows, p=[0.32, 0.26, 0.24, 0.18]),\n",
    "        \"weapon\": rng.choice(weapon_types, size=rows),\n",
    "        \"companion\": rng.choice(companion, size=rows),\n",
    "        \"sessions_played\": rng.integers(5, 120, size=rows),\n",
    "        \"avg_time_alive\": rng.normal(185, 60, size=rows).clip(30, 600),\n",
    "        \"objective_rate\": rng.beta(2.1, 3.5, size=rows),\n",
    "    })\n",
    "    # outcome modelling: burst rifles + medic droni su mappe chiuse funzionano meglio\n",
    "    score = (\n",
    "        0.002 * df[\"skill_rating\"]\n",
    "        + 0.4 * (df[\"weapon\"] == \"burst_rifle\").astype(int)\n",
    "        + 0.35 * (df[\"companion\"] == \"medic_drone\").astype(int)\n",
    "        + 0.25 * (df[\"map\"] == \"Fungal Labyrinth\").astype(int)\n",
    "        + 0.15 * df[\"objective_rate\"]\n",
    "        - 0.001 * df[\"avg_time_alive\"]\n",
    "    )\n",
    "    prob = 1 / (1 + np.exp(-score))\n",
    "    df[\"loadout_success\"] = rng.binomial(1, prob.clip(0.05, 0.95))\n",
    "    return df\n",
    "\n",
    "if DATA_PATH.exists():\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "    source = \"dataset reale\"\n",
    "else:\n",
    "    df = generate_synthetic_dataset()\n",
    "    source = \"dataset sintetico\"\n",
    "\n",
    "print(f\"Dataset origine: {source}\")\n",
    "print(df.head())\n",
    "print(df.describe(include='all').transpose().head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c66e6",
   "metadata": {},
   "source": [
    "## 3. Feature engineering\n",
    "Creiamo le trasformazioni per le feature categoriali e numeriche. Applichiamo scaling ai numeri e one-hot encoding alle categorie per preparare i dati al modello di boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b516b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET = \"loadout_success\"\n",
    "CATEGORICAL_COLS = [\"map\", \"weapon\", \"companion\"]\n",
    "NUMERIC_COLS = [\"skill_rating\", \"sessions_played\", \"avg_time_alive\", \"objective_rate\"]\n",
    "IDENTIFIERS = [\"player_id\"]\n",
    "\n",
    "# Pulizia di base\n",
    "df = df.dropna(subset=CATEGORICAL_COLS + NUMERIC_COLS + [TARGET])\n",
    "df = df.drop_duplicates(subset=IDENTIFIERS + CATEGORICAL_COLS + NUMERIC_COLS)\n",
    "\n",
    "feature_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), CATEGORICAL_COLS),\n",
    "        (\"numeric\", StandardScaler(), NUMERIC_COLS),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "pipeline = Pipeline([(\"features\", feature_transformer), (\"classifier\", model)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d62077",
   "metadata": {},
   "source": [
    "## 4. Train / validation split\n",
    "Utilizziamo un semplice hold-out 80/20 mantenendo la distribuzione della variabile target attraverso stratificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e09c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[CATEGORICAL_COLS + NUMERIC_COLS]\n",
    "y = df[TARGET]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print(f\"Train size: {X_train.shape}, Validation size: {X_valid.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94b702",
   "metadata": {},
   "source": [
    "## 5. Training e valutazione\n",
    "Alleniamo il modello e calcoliamo metriche di accuratezza e AUC ROC per valutarne le prestazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "valid_predictions = pipeline.predict(X_valid)\n",
    "valid_proba = pipeline.predict_proba(X_valid)[:, 1]\n",
    "report = classification_report(y_valid, valid_predictions, output_dict=True)\n",
    "roc_auc = roc_auc_score(y_valid, valid_proba)\n",
    "import json as _json\n",
    "print(_json.dumps(report, indent=2))\n",
    "print({\"roc_auc\": roc_auc})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d7937",
   "metadata": {},
   "source": [
    "## 6. Export modello e metadati\n",
    "Persistiamo il modello con `joblib` e salviamo alcune informazioni utili al servizio di inference (feature, metriche, versione dei dati)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e70204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dump(pipeline, MODEL_OUTPUT)\n",
    "metadata = {\n",
    "    \"model_path\": str(MODEL_OUTPUT.resolve()),\n",
    "    \"features\": {\n",
    "        \"categorical\": CATEGORICAL_COLS,\n",
    "        \"numeric\": NUMERIC_COLS,\n",
    "    },\n",
    "    \"target\": TARGET,\n",
    "    \"metrics\": {\n",
    "        \"classification_report\": report,\n",
    "        \"roc_auc\": roc_auc,\n",
    "    },\n",
    "    \"data_source\": source,\n",
    "    \"row_count\": int(len(df)),\n",
    "}\n",
    "metadata_path = MODEL_OUTPUT.with_suffix('.metadata.json')\n",
    "with metadata_path.open('w', encoding='utf-8') as fp:\n",
    "    json.dump(metadata, fp, indent=2, ensure_ascii=False)\n",
    "print(f\"Modello salvato in {MODEL_OUTPUT}\")\n",
    "print(f\"Metadati salvati in {metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262fe11c",
   "metadata": {},
   "source": [
    "## 7. Prossimi passi\n",
    "- Collegare il notebook a un orchestratore (ad esempio Airflow o Prefect) per schedulare retraining periodici.\n",
    "- Pubblicare gli artefatti su storage condiviso e versionarli (es. MLflow o DVC).\n",
    "- Integrare validazioni aggiuntive sulle feature per intercettare drift nei dati."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
